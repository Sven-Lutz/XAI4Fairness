---
layout: default
title: Research
---

<h2>ğŸ“š Research on Explainable AI and Fairness</h2>
<p>
  This page collects relevant scientific publications on the intersection of
  explainability (XAI) and fairness in algorithmic decision-making. We aim to
  provide both theoretical and practical perspectives through selected
  peer-reviewed papers.
</p>

<hr />

<h3>
  1.
  <a href="https://dl.acm.org/doi/full/10.1145/3613904.3642621" target="_blank"
    >â€œ(Un)Fairness Behind the Curtainâ€ â€“ FAccT 2024</a
  >
</h3>
<p>
  <strong>Authors:</strong> Nuria GonzÃ¡lez-Serrano, Shikun Zhang, Elena Simperl
  et al. <br />
  <strong>Summary:</strong> A study on how explanation types impact fairness
  perceptions in human-AI interaction. The paper reveals that perceived fairness
  depends not only on model output, but also on how explanations are presented.
</p>

<hr />

<h3>
  2.
  <a
    href="https://journals.sagepub.com/doi/abs/10.1111/poms.13839"
    target="_blank"
    >â€œCan AI Be Fair?â€ â€“ Production and Operations Management</a
  >
</h3>
<p>
  <strong>Authors:</strong> Elena Aspris, Amir Gandomi, Marcos LÃ³pez de Prado
  <br />
  <strong>Summary:</strong> This conceptual paper discusses how fairness in AI
  systems should be operationalized, questioning prevailing assumptions and
  proposing a structured classification of fairness types in AI-supported
  decisions.
</p>

<hr />

<h3>
  3.
  <a href="https://ceur-ws.org/Vol-3908/paper_38.pdf" target="_blank"
    >â€œUnderstanding Disparities through XAIâ€ â€“ CEUR WS Vol. 3908</a
  >
</h3>
<p>
  <strong>Authors:</strong> Anonymous authors, Workshop on Trustworthy AI <br />
  <strong>Summary:</strong> The paper explores how different XAI techniques
  (e.g., SHAP, LIME, Counterfactuals) can expose underlying disparities in model
  performance across subgroups. Includes empirical evaluation.
</p>

<hr />

<h3>
  4.
  <a href="https://dl.acm.org/doi/abs/10.1145/3630106.3658990" target="_blank"
    >â€œXAI at the Crossroads: Toward Stakeholder-Aware Explanationsâ€ â€“ CHI
    2024</a
  >
</h3>
<p>
  <strong>Authors:</strong> Min Kyung Lee, Daniel S. Weld et al. <br />
  <strong>Summary:</strong> The authors argue for stakeholder-sensitive design
  in XAI tools and propose a framework that accounts for conflicting needs
  between developers, users, and affected parties.
</p>

<hr />

<h3>ğŸ“¥ Local PDFs (optional)</h3>
<ul>
  <li>
    <a
      href="{{ site.baseurl }}/assets/files/FAccT_UnfairnessCurtain.pdf"
      target="_blank"
      >Download local PDF of Paper 1</a
    >
  </li>
  <li>
    <a href="{{ site.baseurl }}/assets/files/Can_AI_Be_Fair.pdf" target="_blank"
      >Download local PDF of Paper 2</a
    >
  </li>
  <li>
    <a
      href="{{ site.baseurl }}/assets/files/Disparities_XAI_CEUR.pdf"
      target="_blank"
      >Download local PDF of Paper 3</a
    >
  </li>
  <li>
    <a
      href="{{ site.baseurl }}/assets/files/XAI_Stakeholder_CHI.pdf"
      target="_blank"
      >Download local PDF of Paper 4</a
    >
  </li>
</ul>

<p style="font-size: 0.9em; color: #555">
  Please contact us if you'd like to contribute or suggest additional readings.
</p>
